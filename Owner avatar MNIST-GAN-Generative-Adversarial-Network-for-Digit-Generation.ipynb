{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWnh9Ct+jDdQIM/kB7xKhi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MehrdadDastouri/MNIST-GAN-Generative-Adversarial-Network-for-Digit-Generation/blob/main/Owner%20avatar%20MNIST-GAN-Generative-Adversarial-Network-for-Digit-Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDw7sAp03MxB",
        "outputId": "5ae6e110-e3a6-4141-d770-bd6400dee4d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.24MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 154kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.47MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "<urlopen error [Errno 111] Connection refused>\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.38MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/50: 100%|██████████| 938/938 [01:00<00:00, 15.43it/s, d_loss=59.4, g_loss=81.2]\n",
            "Epoch 2/50:  54%|█████▍    | 510/938 [00:33<00:24, 17.67it/s, d_loss=57, g_loss=85.9]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    latent_dim = 100\n",
        "    img_size = (1, 28, 28)\n",
        "    batch_size = 64\n",
        "    epochs = 50\n",
        "    lr = 0.0002\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    sample_dir = \"samples\"\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Linear(1024, 28*28),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        img = self.model(z)\n",
        "        return img.view(img.size(0), *Config.img_size)\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        flattened = img.view(img.size(0), -1)\n",
        "        validity = self.model(flattened)\n",
        "        return validity\n",
        "\n",
        "class GANTrainer:\n",
        "    def __init__(self):\n",
        "        self.config = Config()\n",
        "        self._init_dirs()\n",
        "\n",
        "        # Initialize models\n",
        "        self.generator = Generator(self.config.latent_dim).to(self.config.device)\n",
        "        self.discriminator = Discriminator().to(self.config.device)\n",
        "\n",
        "        # Optimizers\n",
        "        self.optimizer_G = optim.Adam(\n",
        "            self.generator.parameters(), lr=self.config.lr, betas=(0.5, 0.999))\n",
        "        self.optimizer_D = optim.Adam(\n",
        "            self.discriminator.parameters(), lr=self.config.latent_dim, betas=(0.5, 0.999))\n",
        "\n",
        "        # Loss function\n",
        "        self.adversarial_loss = nn.BCELoss()\n",
        "\n",
        "        # Data loader\n",
        "        self.dataloader = self._get_dataloader()\n",
        "\n",
        "    def _init_dirs(self):\n",
        "        os.makedirs(self.config.sample_dir, exist_ok=True)\n",
        "\n",
        "    def _get_dataloader(self):\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5], [0.5])\n",
        "        ])\n",
        "        dataset = torchvision.datasets.MNIST(\n",
        "            root='./data', train=True, download=True, transform=transform)\n",
        "        return DataLoader(\n",
        "            dataset, batch_size=self.config.batch_size, shuffle=True)\n",
        "\n",
        "    def _save_samples(self, epoch):\n",
        "        z = torch.randn(16, self.config.latent_dim).to(self.config.device)\n",
        "        gen_imgs = self.generator(z)\n",
        "        save_image(gen_imgs, os.path.join(\n",
        "            self.config.sample_dir, f\"epoch_{epoch}.png\"), nrow=4, normalize=True)\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.config.epochs):\n",
        "            progress_bar = tqdm(self.dataloader, desc=f'Epoch {epoch+1}/{Config.epochs}')\n",
        "\n",
        "            for i, (imgs, _) in enumerate(progress_bar):\n",
        "                valid = torch.ones(imgs.size(0), 1).to(self.config.device)\n",
        "                fake = torch.zeros(imgs.size(0), 1).to(self.config.device)\n",
        "                real_imgs = imgs.to(self.config.device)\n",
        "\n",
        "                # Train Generator\n",
        "                self.optimizer_G.zero_grad()\n",
        "                z = torch.randn(imgs.size(0), self.config.latent_dim).to(self.config.device)\n",
        "                gen_imgs = self.generator(z)\n",
        "                g_loss = self.adversarial_loss(\n",
        "                    self.discriminator(gen_imgs), valid)\n",
        "                g_loss.backward()\n",
        "                self.optimizer_G.step()\n",
        "\n",
        "                # Train Discriminator\n",
        "                self.optimizer_D.zero_grad()\n",
        "                real_loss = self.adversarial_loss(\n",
        "                    self.discriminator(real_imgs), valid)\n",
        "                fake_loss = self.adversarial_loss(\n",
        "                    self.discriminator(gen_imgs.detach()), fake)\n",
        "                d_loss = (real_loss + fake_loss) / 2\n",
        "                d_loss.backward()\n",
        "                self.optimizer_D.step()\n",
        "\n",
        "                progress_bar.set_postfix(\n",
        "                    g_loss=g_loss.item(), d_loss=d_loss.item())\n",
        "\n",
        "            # Save sample images at each epoch\n",
        "            self._save_samples(epoch)\n",
        "\n",
        "            # Save model checkpoints\n",
        "            if (epoch+1) % 10 == 0:\n",
        "                torch.save(self.generator.state_dict(),\n",
        "                         f\"generator_epoch_{epoch+1}.pth\")\n",
        "                torch.save(self.discriminator.state_dict(),\n",
        "                         f\"discriminator_epoch_{epoch+1}.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    gan_trainer = GANTrainer()\n",
        "    gan_trainer.train()"
      ]
    }
  ]
}